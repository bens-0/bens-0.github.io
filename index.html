<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Consciousness, AI, and what is really real</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="title">
    My Thoughts on Consciousness, AI, and What is Really Real
    </div>
    
    <div class="author">
       by Wojciech Zaremba
    </div>

    <div class="sub-heading">
        Difficult to Discuss, but Just as Fun
    </div>
    
    <div class="text">
        Ever since I was little, I was always fascinated with consciousness.
        I was drawn just how utterly mysterious the nature of this phenomenon was.
        To me, I always saw it as the most mysterious phenomenon, its mysteries ran deeper than even the origin of the universe.
        At least with the origin of the universe, I felt that it might be explained given a more comprehensive understanding of physics…
        Yet for consciousness, we don’t even have basic scientific tools to ask any questions whether or not something is conscious. Some even consider a mosquito or a rock to be conscious, and currently, there is no way to prove or disprove it.
        This thought naturally led my younger-self to ponder: “Could machines have subjective experience? Could they have consciousness?”
        Well, this is my attempt to answer that.
        The following are my conclusions after contemplating the matter endlessly, binging countless books, and observing many parallels in the AI field where I work (I am a co-founder of a well-recognized research organization called OpenAI).
        I have asked myself what is needed for these “thinking” machines to have subjective experience — to be conscious. Versus what would make them intelligent but not having the internal world at all, not feeling, nor smelling, but just intelligently acting. What would make them “Philosophical Zombies”? (more about it later).
        I don’t fully understand the nature of consciousness, nor am I suggesting that AI is conscious (today, we have no tools to prove it even if it really was).
        But, I’m noting the many parallels of the two and drawing a simple conclusion from it based on my knowledge of the nature of consciousness.
        So to start, let’s begin by clarifying what I mean by “consciousness”…
        What is Consciousness?
        This is a question with an answer that is as important as it is difficult.
        The concept has resisted all attempts at conclusive definition throughout history. Despite countless philosophies, religions, and scientific examinations — it continues to defy clear understanding.
        “Official” definitions of consciousness exist (in dictionaries for example), but these run into the same problems of lacking concrete clarity. But, instead of trying to give an academic definition of what consciousness is, I think a simpler way to look at it is by seeing consciousness as equivalent to one’s subjective experiences.
        Think about it, what you experience from moment to moment is completely tied to your consciousness. Every experience you have cannot exceed your consciousness. Inversely, your consciousness is bound by your experiences (no matter how strange or subjective they might be).
        If you were void of consciousness (AKA dead) you can’t have any more experiences (as far as we know), and conversely all experiences in your life whether awake, asleep, on drugs or comatose requires consciousness, no matter how rudimentary.
        Again, I’m not trying to define consciousness in itself (I’ll leave that to the philosophers of the World), but making the claim that consciousness is equivalent to the totality of your experience at any given moment.
        What you experience IS your consciousness — and your consciousness IS your experience. One cannot exist without the other.
        But if consciousness is your subjective experience, what does that do? How can this relate to AI?
        Well before I tie those ideas together, it’s important that we first dig deeper into what makes a subjective experience different from an objective one.
        You’ve Never Experienced Reality
        Subjective experience is simple on the surface.
        These are the experiences we have as individuals that are felt, perceived, understood in our own ways. However, most of us think that within the realm of experiences one can undertake in a lifetime, at least some of it is objective, that at the very least a small portion is a true reflection of the real world.
        Well, I have some bad news for you: we can’t actually have objective experiences.
        In fact, this limitation is something you cannot escape, or turn off — it is something that takes place by definition of having experiences as a living organism.
        But how can this be?
        Well, everything about your experience — sounds, sights, tastes, smell, feelings, moods — every single thing — is passed through a very complex, yet imperfect filter, your mind. Therefore, the reality you are so familiar with isn’t and can’t be the objective reality.
        It is by definition a model of reality.
        Models of Reality: The Ultimate Beer Goggles?
        Take the simple example of the Australian jewel beetle.
        Researchers became interested in this beetle’s behavior when they observed a series of peculiar behaviors where some beetles would consistently clustered around discarded beer bottles in the Outback.
        Upon further research, it became clear to the scientists what produced this strange behavior pattern…
        You see, these congregating beetles were actually males of the species and had clustered around the beer bottles because they confused it for a female member of their species.
        The color, shape, as well as the texture of the bottles resembled the female of their species enough that, to the male Jewel Bettle, it was in every way a member of the opposite sex, ready to mate.
        The beer bottles on the ground provided the correct input to satisfy the male beetle’s model of the world to trigger the idea:
        “This is a mate — now go do what you need to do”
        As funny as this example might be, it reveals that these beetles are operating according to their model of reality; and their model has specific processes that helped them identify a mate.
        More importantly, their model is an imperfect rending of the objective world.
        But what about us? What about humans?
        Our Own Discarded Beer Bottle
        Well, not unlike the Australian jewel beetle, our experiences are also based on our human models of reality. And not unlike their model, our model is also imperfect.
        Of course, we aren’t going to confuse a discarded beer bottle for a potential mate any day soon, but we interact with reality in the same way — that is, through a model.
        And this is the most important idea that I’m presenting here: that you live your life through a model of reality and never directly in contact with it.
        Every experience you’ve ever had was the end result of your mind taking sensory information from the various input sources first (eyes, nose, skin, ears, etc…), then processing it, before finally delivering this experience.
        Each moment of our lives presents us with an incredible volume of information, which our minds then process and condense to fit the model of the world we have.
        The benefit of this is that it allows us to view the world in a comprehensible and structured way. The “downside” (if you will) is that by definition, it means we cannot directly interact with the real world.
        Can our model of reality sometimes closely (or precisely) represent the objective world? Perhaps.
        But this is a question that also can’t be verified.
        However, we can easily verify the contrary — that our model of the world is imperfect.
        I’ll show you here and now that not only do you interpret the world through a model, but that it has glaring gaps.
        Take a look at the image below and especially note the squares marked with “A” and “B”.
        This image is a popular optical illusion, and rightly so.
        The two squares denoted with “A” and “B” are actually the same exact shade and color. Yet, their placements, color contrasts, and shapes warp the modeling machine that is your brain. You cannot help but see them as two different shades.
        When your brain receives all the information presented in the image, your model of reality presents an experience that is divergent from objective reality.
        But what is objective reality in this case?
        Well, it’s quite easy to measure — you can verify it yourself.
        If you have the appropriate tools, you can measure the RGB values of both the “A” and “B” squares and you can see that they are identical. Yet even knowing this empirical fact, you can’t help but see them as two very different shades.
        This is but just one of many examples that show how our subjective experience is not an exact representation of reality, but a model of it as best put together as our brains will allow.
        Despite the inherent flaws of our modeling systems producing subjective experiences that — at times — clearly go against measurable objective facts, our model of the world is still an incredibly complex one.
        But this complexity was not granted to us from the start, it’s origins are notable as we see very similar parallels during the advancement of AI.
        How did we get our model of the world?
        This is quite a complex question and I am by no means an expert.
        That said, I believe our ability to model the world is largely a direct product of evolution and all that it shapes within us.
        As bold as this statement might be, hear me out.
        Evolution is the force and mechanism that allows organisms (like us) to pass on hereditary traits across successive generations. Now I’m sure I’m doing a great disservice to the complexity of evolution by distilling it down to a single sentence, but bear with me here…
        As a product of evolution, the DNA of all living beings is continually shaped and reshaped into ever more efficient ways to allow the organism to not just survive…but also thrive.
        It is this continuous and iterative process that slowly creates our model of the world until it is what we have today.
        Really, if you think about it, the model is a way for us to distill all the information we constantly get, then provide it in a way that allows us to navigate this incredibly complex world efficiently.
        Thanks to this evolving model, we as a species, have (largely) succeeded in avoiding being eaten by animals, or being more attentive to bright colors for ripe fruit, or detecting minor movements in facial features. All of the things that make our perception of the world what it is today is because of this model in our minds.
        Make no mistake — this was a long journey to get here.
        But each step of the iterative improvement to our model of reality was oriented towards what can best be perceived to help us survive and thrive.
        Therefore, even when starting with a very basic model of the world, given a basic drive and enough iteration, we are living proof that very complex models of the world can emerge over time.
        It’s important to remember that I’m not trying to explain the origin of humans, nor am I making a claim about what intelligence really is. Instead, I am broadly remarking on the mechanisms that allow ever more complex models of reality to emerge.
        And what is most remarkable about this process is that we are seeing many of the same mechanisms and processes taking place within the work we’ve been doing with AI.
        AI’s Subjective Model
        When it comes to artificial intelligence, we’ve been able to observe many similar patterns of development when it comes to the models of the world produced based on the “objective” worlds AI inhabit.
        One of the most notable examples can be seen here:
        “Training an agent to drive a car blindfolded, but occasionally letting it see.
        The above animation illustrates the world model’s prediction on the right versus the ground truth pixel observation on the left. Frames that have a red border frames indicate actual observations from the environment the agent is allowed to see. The policy is acting on the observations (real or generated) on the right.” link
        This image comes from an AI used for simulated car driving. The AI agent here is being trained to recognize important features in the world it will operate in.
        On the left you see what information was fed to the agent — there are typical features outlined here like roads, the “grass” or what is not the road, and other obstacles on the road.
        Over many iterations of learning, the AI agent has produced its own model of the world that is a representation but not an exact copy of the “real world” that it will inhabit. In other words, if the world in which the Agent will inhabit is the objective reality, then what you see on the right, is how the Agent sees its world based on the model.
        But even if we can see the agent’s model of the world clearly differing from its objective reality, how does it arrive at this model?
        Well much like how humans develop ever more complex models of the world we inhabit, AI Agents function much in the same way. And perhaps the most striking example of this whole process was the work we did when training agents play the game of DOTA 2.
        DOTA 2: AI Zero to AI Hero
        DOTA 2, or “Defense Against the Ancients 2” is a very popular competitive computer game.
        In DOTA, two opposing teams are positioned inside a map with boundaries at opposite ends. Each team is tasked with trying to bring down the base of the other team. To accomplish this, each team must fight their way through a number of defensive structures and the members of the opposing team.
        Each team consists of 5 players and the members work together to achieve the same high level objective — to defeat the other team while protecting your own base.
        Every player selects and controls one unit, known as a “hero” — each hero is unique in both how they play and what they can do — and there are 119 of them (for now).
        Therefore each team of 5 heroes will work together, utilizing their unique strengths to try and overpower the other team of 5 with the defeat of the opponent’s base being the end goal.
        There are many nuances I’m leaving out here, but even with these parameters, you can already see that the game itself is quite complex. In fact, the game has such a high skill ceiling that yearly international tournaments are held with the best teams in the world competing for massive cash prizes (in 2019, with a prize pool of $30 million!)
        So, seeing the complex domain this game occupies — we set out with an ambitious goal to have an AI Agent try and master it. Given the sheer number of considerations and possibilities, the only way was to have the AI learn the game by itself and develop strategies on its own.
        Try Try Try Again
        We began by giving agents a basic set of parameters and incentives.
        They could get information that would only be available to a human player at any moment, and they were incentivized to win and decentivized from losing.
        Then with the help of reinforcement learning, these agents were trained over thousands of years of simulated gameplay that continually helped shape their model of the game world to ever more complex forms from which advanced strategies would be used.
        Over the course of our training, we saw the AI agents go from doing totally random things to being able to play this game at the highest levels, and the journey was incredible.
        At the very start, the agent controlled heroes were essentially wandering aimlessly, not really “playing” the game. They had some basic incentives to win, but achieving this objective was not something the agents knew how to do…yet.
        At this point, it would be fair to say that the agent’s model of the game world was primitive and not-at-all suited for the task of victory. However, this changed and it changed fast.
        As we trained the agents more and more, we could observe that the agent’s model of its world grew increasingly more sophisticated. Before long, iterations of the AI agent began to exhibit behavior that was only possible if its model of the world allowed for such complexities.
        For instance, there came a time when the agents knew when to attack a weaker enemy and when to back off in the face of superior force. This shows a fundamental grasp of the enemies in the game.
        Soon after, we observed agents coordinating with other agents to execute exceptionally complex strategies — going as far as “tricking” the opponents by feigning weakness or leading them into traps.
        Each of these strategic moves first required the agent’s model of reality to grow to a point where these concepts could be understood, and only after this point could these strategies be made.
        These days the AI Agents playing DOTA have such sophisticated models of their world that they are able to play competitively against the best players in the world — a far cry from the unimpressive early strategies of “wandering aimlessly”.
        And though these agents are bound by the game world of DOTA, the process that shaped their models has striking resemblance to our own.
        AI Has Consciousness (or maybe not)
        So what’s my point with all of this?
        Well, it’s that AI might already be conscious, and we don’t even know it yet.
        If consciousness is the totality of our subjective experience of the world, and if this subjective experience is produced through a model, and if we assume this model rises in complexity over time…
        Then AI could have a very basic form of consciousness already.
        Notice how I emphasized “could”?
        We actually can’t confirm this in the AI systems we’ve built today.
        But, this problem isn’t anything new, this is an age old conundrum affecting humans for as long as we’ve been conscious. It has actually been wrapped up into a nice thought experiment known as “Philosophical Zombie.”
        The quick summary of the idea is that every person you meet could appear, feel and respond indistinguishable from a human, yet lack any real consciousness, thereby making them a “zombie” of sorts.
        Take for example the experience of seeing the color red.
        Assume that I find that color very appealing and therefore feel joy when viewing it. Well, it’s quite easy to verify the stimulus and my expected response (show me something red, and check if I’m feeling joy), but it’s impossible to verify if I actually experienced the color red.
        This experiencing part is known as the “phenomenal consciousness” while the stimulus/response part is known as “functional consciousness.” The difficulty in determining the existence of phenomenal consciousness has been dubbed as the hard problem of consciousness by David Chalmers.
        
        This inability to prove consciousness is the same problem we face with AI. Despite being able to verify that the input matches the output, and that behavior patterns are showing signs of “intelligence”, we cannot measure whether the AI agent has actually experienced it…or is just some sort of robotic zombie.
        But the similarities between consciousness in humans and AI doesn’t end there, because even the steps taken to develop our advanced models of the world also has parallels in AI.
        AI has already shown us many times that extremely complex behaviors can arise from very simple inputs. Given enough iterations, a simple model of a world can grow ever more complex — continuously forming more advanced concepts as time goes on.
        It is from this advanced model of the world that we can observe progressively more complex behaviors that ultimately achieve the underlying goals (sometimes in totally unprecedented ways).
        You can probably already see how this development closely parallels our own.
        Starting with our original ancestors as single-celled bacterium “aimlessly wandering” the primordial sea; and propelled by their sole mission to thrive and survive. Given enough iterations (in this case 4.54 billion years worth), it is easy to imagine that the models of the world that our ancient ancestors used to navigate their world continuously grew to what we have today.
        Nothing is Sacred
        In fact “the concept of self”- one of the bastions that humans hold dear as something uniquely human — is just another rung on the ladder of complex world models. Although this concept sits atop the peak of “the things that make us human”, it’s origins aren’t as mysterious as one might think.
        If we take the example of an ancient single celled organism having the most basic model of the world, then it would only be a matter of time before it’s model grew more complex in response to the rising complexity of the world it dealt with.
        For instance, at some point depth perception (distinguishing foreground from background) became necessary in the model for survival, and after many iterations the model grew to distinguish predators from prey, then the model could handle 3D vision, then color perception, etc…
        It is then conceivable (and very probable) that our model’s concept of “self” developed as a necessity to help us survive and thrive in an ever more complex world. As having the idea of “self” enables us to solve incredibly complex problems.
        Final Thoughts
        So where are we now? What was my point in all of this?
        Well simply to run through some ideas I had bouncing in my mind for some time now and lay it out in a (hopefully) organized manner.
        If there’s anything to be taken away from the reading it’s that:
        Consciousness and especially the concept of self are fixtures of what makes humans “special”; and our very complex and still flawed model of the world gives us these gifts, as well as shape how we perceive and interact with the world.
        <br>
        <br>
        Yet, the biological mechanism working underneath to produce these features seemingly function the same (or very similar) to how an AI’s model develops. And therefore it is not inconceivable that AI already has a rudimentary form of consciousness.
        However, the inability to prove consciousness in others leaves this an open question for discussion and further thought. And the inner child in me looks forward to every new development in this regard.
        <br>
        <br>
        Thanks for reading.
        <br>
        <br>
        <br>
    </div>
    
    
    
</body>
</html>